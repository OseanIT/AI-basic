{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. 신경망 학습 (Training Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습문제**: 본 문서에 나오는 파이썬/넘파이 스크립트에서 None과 pass로 표시된 곳을 찾아서 적절한 코드로 대체하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 데이터에서 학습한다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망(딥러닝)은 종단간 기계학습(end-to-end machine learning)을 가능하게 함\n",
    "- 입력 데이터에서 목표한 결과(출력)를 사람의 개입 없이 얻을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig-4-2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**훈련 데이터 (training data) vs 시험 데이터 (test data)**:\n",
    "\n",
    "- training data : 최적의 매개변수를 찾기위한 학습용\n",
    "- test data : 훈련한 신경망 모델을 평가 (모델의 범용능력을 평가)\n",
    "- overfitting(과적합) : 신경망 모델이 훈련 데이터만 잘 예측(분류)하고, 시험 데이터에 대해서는 잘 못하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 손실 함수 (Loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 평균 제곱 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한개의 데이터에 대한 손실함수:\n",
    "<img src=\"e-4.1.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])    # one-hot encoding\n",
    "np.concatenate((y.reshape(1,-1), t.reshape(1,-1)))      #np.r_[y.reshape(1,-1),t.reshape(1,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)     # 여기에 None 대신에 적절한 코드를 입력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])    # one-hot encoding\n",
    "\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])      # '2'일 확률이 가장 높다고 추정함 (0.6)\n",
    "mean_squared_error(y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])      # '7'일 확률이 가장 높다고 추정함 (0.6)\n",
    "mean_squared_error(y,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한개의 데이터에 대한 손실함수:\n",
    "<img src=\"e-4.2.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7    # 10^(-7)\n",
    "    return -np.sum(t * np.log(y+delta))      # log = log_e    # 여기에 None 대신에 적절한 코드를 입력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])    # one-hot encoding\n",
    "\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])      # '2'일 확률이 가장 높다고 추정함 (0.6)\n",
    "cross_entropy_error(y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])      # '7'일 확률이 가장 높다고 추정함 (0.6)\n",
    "cross_entropy_error(y,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N 개의 데이터에 대한 손실함수:\n",
    "<img src=\"e-4.3.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치(mini-batch) 학습: 전체 데이터 중에 일부만 골라 학습 (예, 60,000 장의 훈련 데이터 중에서 100장을 무작위로 뽑아서 100장만을 사용하여 학습 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)    # (60000, 784)\n",
    "print(t_train.shape)    # (60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60,000 개의 훈련 데이터로부터 10개만 무작위로 골라 낼려면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.choice(60000,10)      # 0 ~ 59999 사이의 정수 중에 무작위로 10개를 골라냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.choice()로부터 나온 10개의 인덱스를 이용해 x_train과 t_train으로부터 10개의 데이터를 뽑아 냄:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "#x_batch, t_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 배치용 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):    # t가 one-hot encoding인 경우만을 가정함, t가 숫자 레이블인 경우는 교재 참조\n",
    "    if y.ndim == 1:      # 데이터가 하나만 있는 경우\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta)) / batch_size    # np.sum() 하나로 2차원 행렬 t와 np.log(y)의 모든 원소 간의 곱을 더함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])    # one-hot encoding\n",
    "\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])      # '2'일 확률이 가장 높다고 추정함 (0.6)\n",
    "cross_entropy_error(y,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전방차분:\n",
    "$$ \\frac{df(x)}{dx} = \\lim\\limits_{h \\to 0} \\frac{f(x+h)-f(x)}{h} $$\n",
    "중앙차분:\n",
    "$$ \\frac{df(x)}{dx} = \\lim\\limits_{h \\to 0} \\frac{f(x+h)-f(x-h)}{2h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수치미분 구현시에는 전방 차분 보다는 중앙 차분으로 하는 것이 오차를 줄일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):    # 고차 함수 : 함수를 인자로 받는 함수\n",
    "    h = 1e-4    # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**수치미분의 예**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**편미분과 gradient**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    '''''\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "    '''''\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig-4-8.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습문제**: 위의 그래프를 출력하는 파이썬/넘파이 스크립트를 작성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x0 = np.linspace(-3, 3, 30)     # my insert\n",
    "x1 = np.linspace(-3, 3, 30)\n",
    "xn = x0.shape[0]\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "y = np.zeros((len(x0),len(x1)))\n",
    "X = np.zeros(2)\n",
    "for i1 in range(xn):\n",
    "    for i0 in range(xn):\n",
    "        X[0] = X0[i1,i0]\n",
    "        X[1] = X1[i1,i0]\n",
    "        y[i1,i0] = function_2(X)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X0, X1, y, rstride=1, cstride=1, alpha=0.3,\n",
    "                color='w', edgecolor='black')\n",
    "ax.view_init(25, -115) \n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient at $(x_0=3, x_1=4) : (\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1})$\n",
    "- 모든 편미분을 벡터화 한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#def _numerical_gradient_no_batch(f, x):\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)    # x와 형상이 같고 그 원소가 모두 0인 배열을 생성함\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)  # f(x+h)\n",
    "        #print(fxh1)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x)  # f(x-h)\n",
    "        #print(fxh2)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val  # 값 복원\n",
    "\n",
    "        print(x[idx], grad[idx])\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient를 그려보기:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig-4-9.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습문제**: 위의 그림과 같이 gradient를 그리는 파이썬 스크립트를 작성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val          \n",
    "    return grad\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad\n",
    "def function_2(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_gradient(f, x)\n",
    "    #print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "if __name__ == '__main__':\n",
    "    x0 = np.arange(-2, 2.5, 0.25)\n",
    "    x1 = np.arange(-2, 2.5, 0.25)\n",
    "    X, Y = np.meshgrid(x0, x1)\n",
    "    \n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "\n",
    "    grad = numerical_gradient(function_2, np.array([X, Y]).T).T\n",
    "\n",
    "    plt.figure()\n",
    "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.grid()\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 경사 하강법 (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"e-4.7.png\" width=\"200\">     $\\eta$: 학습률(learning rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습문제**: 경사 하강법으로 아래 그림과 같이 $f(x_0,x_1)=x_0^2+x_1^2$의 최솟값을 구하라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig-4-10.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "x0 = np.linspace(-5, 5, 100)     \n",
    "x1 = np.linspace(-5, 5, 100)\n",
    "xn = x0.shape[0]\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "y = np.zeros((len(x0),len(x1)))\n",
    "X = np.zeros(2)\n",
    "for i1 in range(xn):\n",
    "    for i0 in range(xn):\n",
    "        X[0] = X0[i1,i0]\n",
    "        X[1] = X1[i1,i0]\n",
    "        y[i1,i0] = function_2(X)\n",
    "\n",
    "plt.figure(1, figsize=(5, 5))\n",
    "cont = plt.contour(X0, X1, y, levels=(0.25, 1, 4, 9, 16), colors='black', alpha=0.5, linestyles='dashed')    \n",
    "plt.xlim(-4.5, 4.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률이 너무 크면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률이 너무 큰 예 : lr=10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "#gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률이 너무 작으면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률이 너무 작은 예 : lr=1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "#gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**파라미터 vs 하이퍼 파라미터**:\n",
    "\n",
    "- 파라미터 : W, b - 학습 알고리즘에 의해서 자동 획득\n",
    "- 하이퍼 파라미터 : 학습률, 반복횟수, 미니배치 크기 등 - 사람이 직접 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"e-4.8.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**간단한 신경망 simpleNet 구현 예:** 1 layer NN\n",
    "\n",
    "- 입력 X = [x0,x1]\n",
    "- 중간 출력 Z = [z1,z2,z3] = X*W      where W = 2x3 matrix\n",
    "- 출력 Y = [y0,y1,y2]  = softmax(Z)\n",
    "- 정답 T = [t0,t1,t2]      => one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpleNet 클래스 정의:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉토리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error    # common library 사용\n",
    "from common.gradient import numerical_gradient        # common library 사용\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)        # 가중치 매개변수 초기화 - 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpleNet 테스트:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)    # 가중치 매개변수 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argmax(p)    # 최대값의 인덱스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss 계산:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1])    # 정답 레이블\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기(gradient) 계산:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f(W):  # 인수 W는 dummy로 만든 것?\n",
    "           # net.loss() 함수는 W에 따라서 그 값이 달라짐. 따라서 f()는 W의 함수임\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)    # 고차함수 : 함수의 인자로 함수를 넘김\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 함수 f를 람다 식으로 표현하면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**간단한 신경망 simpleNet 구현 예 끝:** \n",
    "\n",
    "- 이 예제에서는 신경망의 기울기(gradient) 까지만 구함. \n",
    "- 경사하강법에 따라 가중치 매개변수를 갱신하는 학습 알고리즘은 다음 절에서 설명함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습문제**: MNIST 데이터에 대하여 1층 신경망을 구성하고 매개변수를 학습시켜서 학습시간과 정확도를 측정하시오. 머신러닝 교과서 '6장 Logistic Regression'과 본 교재 4.5절 2층 신경망 구현 내용을 참조하여 위의 simpleNet을 적절히 수정하여 구현하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST에 대한 OneLayerNet 클래스 정의: (머신러닝 교과서 '6장 Logistic Regression, 6.3절 2차원 입력 3클래스 분류'과 본 교재 4.5절 2층 신경망 구현 내용 참조)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network configuration :\n",
    "- input layer neurons: 784\n",
    "- output layer neurons: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A^{(1)} = X \\cdot W^{(1)} + B^{(1)} $$\n",
    "\n",
    "$$ \\begin{pmatrix} a_1^{(1)} & \\cdots & a_{10}^{(1)} \\end{pmatrix} = \n",
    "   \\begin{pmatrix} x_1 & \\cdots & x_{784} \\end{pmatrix} \n",
    "   \\begin{pmatrix} w_{1,1}^{(1)} & \\cdots & w_{10,1}^{(1)} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{1,784}^{(1)} & \\cdots & w_{10,784}^{(1)} \\end{pmatrix}\n",
    "   + \\begin{pmatrix} b_1^{(1)} & \\cdots & b_{10}^{(1)} \\end{pmatrix} $$\n",
    "   \n",
    "$$ Y = softmax(A^{(1)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습해야 할 매개변수 개수:\n",
    "\n",
    "- W1 = 784*10, b1 = 10, Total = 7850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneLayerNet 클래스 정의:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error  \n",
    "from common.gradient import numerical_gradient   \n",
    "\n",
    "class OneLayerNet:\n",
    "    def __init__(self, input_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, output_size)\n",
    "        self.params['b1'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1 = self.params['W1']\n",
    "        b1 = self.params['b1']    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        y = softmax(a1)        \n",
    "        return y        \n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):  \n",
    "        loss_W = lambda W: self.loss(x, t)        \n",
    "        grads = {}\n",
    "        print(\"Call numerical_gradient for W1\\n\") \n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        print(\"Call numerical_gradient for b1\\n\")    \n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t): \n",
    "        W1 = self.params['W1']\n",
    "        b1 = self.params['b1']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        y = softmax(a1)        \n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W1'] = np.dot(x.T, dy)\n",
    "        grads['b1'] = np.sum(dy, axis=0)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneLayerNet 테스트:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = OneLayerNet(input_size=784, output_size=10)\n",
    "print(\"net.params['W1'].shape = \", net.params['W1'].shape)\n",
    "print(\"net.params['b1'].shape = \", net.params['b1'].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) \n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient 계산:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)   \n",
    "t = np.random.rand(100, 10)  \n",
    "\n",
    "grads = net.numerical_gradient(x, t) \n",
    "    \n",
    "print(\"grads['W1'].shape = \", grads['W1'].shape)  \n",
    "print(\"grads['b1'].shape = \", grads['b1'].shape) \n",
    "\n",
    "print(\"grads['W1'] = \", grads['W1'])    \n",
    "print(\"grads['b1'] = \", grads['b1'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치 학습 구현하기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "onelayernet = OneLayerNet(input_size=784, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "for i in range(iters_num):\n",
    "    \n",
    "    batch_mask = np.random.choice(train_size, batch_size)    \n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = onelayernet.gradient(x_batch, t_batch)    \n",
    "    \n",
    "    for key in ('W1', 'b1'):\n",
    "        onelayernet.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = onelayernet.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "print(\"Mini-batch training Done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시험 데이터로 평가하기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "onelayernet = OneLayerNet(input_size=784, output_size=10)\n",
    "\n",
    "iters_num = 10000 \n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = onelayernet.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1'):\n",
    "        onelayernet.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = onelayernet.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = onelayernet.accuracy(x_train, t_train)\n",
    "        test_acc = onelayernet.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"i = \" + str(i) + \" | train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "눈으로 확인하기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline  \n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    imshow(np.asarray(pil_img)) \n",
    "\n",
    "test_size = x_test.shape[0]\n",
    "test_mask = np.random.choice(test_size, 10)\n",
    "print(\"test_mask = \", test_mask)\n",
    "\n",
    "x = x_test[test_mask]\n",
    "t = t_test[test_mask]\n",
    "y = onelayernet.predict(x)\n",
    "\n",
    "y = np.argmax(y, axis=1)\n",
    "t = np.argmax(t, axis=1)\n",
    "\n",
    "print(\"y =\", end=\" \")\n",
    "for i in range(10):\n",
    "    label = y[i]\n",
    "    print(label, end=\"            \") \n",
    "\n",
    "print(\"t =\", end=\" \")\n",
    "for i in range(10):\n",
    "    label = t[i]\n",
    "    print(label, end=\"            \") \n",
    "        \n",
    "plt.figure(figsize=(15, 2))\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5) \n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    img = x[i]*256      \n",
    "    img = img.reshape(28, 28)  \n",
    "    img_show(img)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 2층 신경망에 대한 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> $w_{ij} = w_{ij} - \\eta \\frac{\\partial L}{\\partial w_{ij}}$ </center>\n",
    "\n",
    "- 데이터를 미니배치로 무작위로 선정\n",
    "- 확률적 경사 하강법 (SGD: Stochastic Gradient Descent) : 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사하강법\n",
    "\n",
    "**신경망 학습의 절차**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"p180.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network configuration :\n",
    "- input layer neurons: 784\n",
    "- hidden layer neurons: 50\n",
    "- output layer neurons: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A^{(1)} = X \\cdot W^{(1)} + B^{(1)} $$\n",
    "\n",
    "$$ \\begin{pmatrix} a_1^{(1)} & \\cdots & a_{50}^{(1)} \\end{pmatrix} = \n",
    "   \\begin{pmatrix} x_1 & \\cdots & x_{784} \\end{pmatrix} \n",
    "   \\begin{pmatrix} w_{1,1}^{(1)} & \\cdots & w_{50,1}^{(1)} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{1,784}^{(1)} & \\cdots & w_{50,784}^{(1)} \\end{pmatrix}\n",
    "   + \\begin{pmatrix} b_1^{(1)} & \\cdots & b_{50}^{(1)} \\end{pmatrix} $$\n",
    "   \n",
    "$$ Z^{(1)} = sigmoid(A^{(1)}) $$\n",
    "   \n",
    "$$ A^{(2)} = Z^{(1)} \\cdot W^{(2)} + B^{(2)} $$\n",
    "\n",
    "$$ \\begin{pmatrix} a_1^{(2)} & \\cdots & a_{10}^{(2)} \\end{pmatrix} = \n",
    "   \\begin{pmatrix} z_1^{(1)} & \\cdots & z_{50}^{(1)} \\end{pmatrix} \n",
    "   \\begin{pmatrix} w_{1,1}^{(2)} & \\cdots & w_{10,1}^{(2)} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{1,50}^{(2)} & \\cdots & w_{10,50}^{(2)} \\end{pmatrix}\n",
    "   + \\begin{pmatrix} b_1^{(2)} & \\cdots \\ b_{10}^{(2)} \\end{pmatrix} $$\n",
    "   \n",
    "$$ Y = softmax(A^{(2)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습해야 할 매개변수 개수:\n",
    "\n",
    "- W1 = 784*50, b1 = 50\n",
    "- W2 = 50*10,  b2 = 10\n",
    "- Total = 39,760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet 클래스 정의:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉토리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient      # 2019/10/13 use numerical_gradient in common\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        print(\"Call numerical_gradient for W1\\n\")    # 2019/10/13 for test\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        print(\"Call numerical_gradient for b1\\n\")    # 2019/10/13 for test\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        print(\"Call numerical_gradient for W2\\n\")    # 2019/10/13 for test\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        print(\"Call numerical_gradient for b2\\n\")    # 2019/10/13 for test\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet 클래스가 사용하는 변수와 메소드:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tab-4-1.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet 테스트:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "print(\"net.params['W1'].shape = \", net.params['W1'].shape)    # (784, 50)\n",
    "print(\"net.params['b1'].shape = \", net.params['b1'].shape)    # (50,)\n",
    "print(\"net.params['W2'].shape = \", net.params['W2'].shape)    # (50, 10)\n",
    "print(\"net.params['b2'].shape = \", net.params['b2'].shape)    # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)    # 더미 입력 데이터 (100장 분량)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient 계산:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 시간이 오래 걸리니 참고 기다리세요 !!!\n",
    "\n",
    "x = np.random.rand(100, 784)    # 더미 입력 데이터 (100장 분량)\n",
    "t = np.random.rand(100, 10)     # 더미 정답 레이블 (100장 분량)  => one-hot encoding이 아님 \n",
    "\n",
    "grads = net.numerical_gradient(x, t)    # 기울기 계산\n",
    "    \n",
    "print(\"grads['W1'].shape = \", grads['W1'].shape)    # (784, 50)\n",
    "print(\"grads['b1'].shape = \", grads['b1'].shape)    # (50,)\n",
    "print(\"grads['W2'].shape = \", grads['W2'].shape)    # (50, 10)\n",
    "print(\"grads['b2'].shape = \", grads['b2'].shape)    # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉토리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "twolayernet = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "iters_num = 10000  # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #print(str(i) + \"-th iteration\")\n",
    "    \n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)    # 0 ~ 60000-1 숫자 중에 랜덤하게 100개를 선택\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = twolayernet.numerical_gradient(x_batch, t_batch)    # 너무 느려서 사용하기 곤란 !!!\n",
    "    grad = twolayernet.gradient(x_batch, t_batch)      # numerical gradient 대신에 analytical gradient 사용, 구현은 5장에서 \n",
    "    \n",
    "    # 매개변수 경신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        twolayernet.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = twolayernet.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "print(\"Mini-batch training Done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수 값의 추이 : 왼쪽은 10,000회 반복까지의 추이, 오른쪽은 1,000회 반복까지의 추이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![대체 텍스트](fig-4-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습문제**: 위의 그래프를 출력하는 파이썬 스크립트를 작성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 시험 데이터로 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉토리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "twolayernet = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "iters_num = 10000  # 繰り返しの回数を適宜設定する\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1 에폭당 반복수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "# 1 에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당합니다.\n",
    "# 예컨데 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, \n",
    "# 확률적 경사하강법을 100회 반복하면 모든 훈련 데이터를 소진하게 됩니다.\n",
    "# 이 경우 100회가 1 에폭이 됩니다.\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = twolayernet.numerical_gradient(x_batch, t_batch)\n",
    "    grad = twolayernet.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        twolayernet.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습경과 기록\n",
    "    loss = twolayernet.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1 에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = twolayernet.accuracy(x_train, t_train)\n",
    "        test_acc = twolayernet.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"i = \" + str(i) + \" | train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfiting(과적합): 훈련 데이터에 포함된 이미지만 제대로 구분하고, 테스트 데이터 등 다른 이미지는 잘 구분해 내지 못하는 현상\n",
    "\n",
    "- 위의 예제는 오버피팅 없이 훈련 데이터와 테스트 데이터에 모두 잘 동작하고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 눈으로 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습문제**: 아래 그림과 같이 TwoLayerNet이 잘 동작하는지 테스트 데이터에서 무작위로 10개를 추출하여 prediction 한 결과를 눈으로 확인하는 프로그램을 작성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline  \n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    imshow(np.asarray(pil_img)) \n",
    "\n",
    "test_size = x_test.shape[0]\n",
    "test_mask = np.random.choice(test_size, 10)\n",
    "print(\"test_mask = \", test_mask)\n",
    "\n",
    "x = x_test[test_mask]\n",
    "t = t_test[test_mask]\n",
    "y = twolayernet.predict(x)\n",
    "\n",
    "y = np.argmax(y, axis=1)\n",
    "t = np.argmax(t, axis=1)\n",
    "\n",
    "print(\"y =\", end=\" \")\n",
    "for i in range(10):\n",
    "    label = y[i]\n",
    "    print(label, end=\"            \") \n",
    "\n",
    "print(\"t =\", end=\" \")\n",
    "for i in range(10):\n",
    "    label = t[i]\n",
    "    print(label, end=\"            \") \n",
    "        \n",
    "plt.figure(figsize=(15, 2)) \n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    img = x[i]*256  \n",
    "    img = img.reshape(28, 28)  \n",
    "    img_show(img)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
